# -*- coding: utf-8 -*-
import pandas as pd
import torch
from sklearn.metrics import log_loss, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn import metrics
import os
import math
import torch.nn as nn
import numpy as np
import torch.utils.data as Data
from torch.utils.data import DataLoader
import torch.optim as optim
import torch.nn.functional as F
from sklearn.metrics import log_loss, roc_auc_score,accuracy_score
from collections import OrderedDict, namedtuple, defaultdict
import random

from Graph_DeepFM import Graph_Deepfm
#from Graph_Wide&Deep import Graph_WideDeep
import matplotlib.pyplot as plt

def data_pre(raw):
    cols = raw.columns.values
    dense_feats = [f for f in cols if f[0] == 'I']
    sparse_feats = [f for f in cols if f[0] == 'C']

    def process_dense_feats(data, feats):
        d = data.copy()
        d = d[feats].fillna(0.0)
        for f in feats:
            d[f] = d[f].apply(lambda x: np.log(x + 1) if x > -1 else -1)
        return d

    data_dense = process_dense_feats(raw, dense_feats)

    def process_spares_feats(data, feats):
        d = data.copy()
        d = d[feats].fillna('-1')
        for f in feats:
            d[f] = LabelEncoder().fit_transform(d[f])
        return d

    def OneHot(data, feats):
        d = data.copy()
        sum1 = 0
        for feat in feats:
            if len(d[feat].unique()) > 100:
                d = d.drop(feat, axis=1)
            else:
                sum1 += 1
                d = pd.concat([d, pd.get_dummies(d[feat], prefix=feat)], axis=1)
                d = d.drop(feat, axis=1)
        return d

if __name__ == "__main__":

    # "your_data.csv" is consistent with the dataset constructed from graph topology.
    data_raw = pd.read_csv('your_data.csv')
    data = data_pre(data_raw)
    train_data = data.iloc[:, 0:-1].to_numpy()
    label = le.fit_transform(data_raw.iloc[:,-1].to_numpy())
    dense_feats = [f for f in cols if f[0] == 'I'] # 'I' represents numeric features
    sparse_feats = [f for f in cols if f[0] == 'C']  # 'C' represents class features,c is the number of categories after one-hot encoding the categorical columns 
    col_names = dense_features + sparse_features +['label']
    data.columns = col_names
  
    num_features = len(data.columns)
    #pooling layer parameter ratio.
    ratio = 0.4
    new_features_length =math.ceil(ratio * num_features)

    new_features= ['I' + str(i) for i in range(len(dense_feats), len(dense_feats)+new_features_length)]
    for feat in new_features:
        data[feat] = np.zeros((data.shape[0], 1))
        dense_features.append(feat)

    feature_names =   sparse_features +dense_features      
    data[sparse_features] = data[sparse_features].fillna('-1', )   # Missing categorical feature, replaced with -1
    data[dense_features] = data[dense_features].fillna(0, )        # Missing numeric feature, replaced with 0
    target = ['label']                                            


    # Using LabelEncoder(), assign a unique number to each item of the categorical feature
    for feat in sparse_features:
        lbe = LabelEncoder()
        data[feat] = lbe.fit_transform(data[feat])

    # Normalization of numeric features to the range 0-1 using max-min scaling.
    mms = MinMaxScaler(feature_range=(0, 1))
    data[dense_features] = mms.fit_transform(data[dense_features])
    feat_sizes1={ feat:1 for feat in dense_features}
    feat_sizes2 = {feat: len(data[feat].unique()) for feat in sparse_features}
    feat_sizes={}
    feat_sizes.update(feat_sizes2)
    feat_sizes.update(feat_sizes1)

    train, test = train_test_split(data,data_raw['label'], test_size=0.25,random_state=2020)

    train_model_input = {name: train[name] for name in feature_names}
    test_model_input =  {name: test[name]  for name in feature_names}

    device = 'cpu'
    use_cuda = True
    if use_cuda and torch.cuda.is_available():
        print('cuda ready...')
        device = 'cuda:0'

    #Retrieve edges and read the graph structure generated by the Graph Structure Learning file
    edge = pd.read_csv("Graph.txt", sep=' ', header=None)
    edge = edge.values
    arr_edge = np.zeros((data.shape[1] - 1, data.shape[1] - 1))
    for i in range(len(edge)):
        arr_edge[int(edge[i][0])][int(edge[i][1])] = 1
        arr_edge[int(edge[i][1])][int(edge[i][0])] = 1
    from scipy.sparse import coo_matrix
    arr_edge1 = coo_matrix(arr_edge)
    edge_index = [arr_edge1.row, arr_edge1.col]
    edge_index = torch.tensor(edge_index, dtype=torch.long)
    #Invoke Graph_DeepFM
    model = Graph_Deepfm(feat_sizes,sparse_feature_columns = sparse_features,dense_feature_columns = dense_features,
                    dnn_hidden_units=[512, 256, 512], dnn_dropout=0, ebedding_size=j,
                    )
    # model = Graph_WideDeep(feat_sizes, ratio, new_features_length, num_features, sparse_features, dense_features,
    #                  [512, 512, 512], dnn_dropout=0.0, embedding_size=10, device=device, l2_reg_dnn=1,
    #                  edge_index=edge_index,
    #                  batch_size=1024)

    model.fit(train_model_input, train[target].values , test_model_input , test[target].values , epochs=200, verbose=10)
    total_params = sum(p.numel() for p in model.parameters())
    total_size = total_params * 4 / (1024 ** 2)  # in MB

    pred_ans = model.predict(test_model_input, 256)
    end = time.time() - since
    pred = np.where(pred_ans > 0.5, pred_ans, 0)
    pred = np.where(pred < 0.5, pred, 1)


    print(round(log_loss(test[target].values, pred_ans), 4),
            round(roc_auc_score(test[target].values, pred_ans), 4),
            round(accuracy_score(test[target].values, pred), 4),
            round(metrics.f1_score(test[target].values, pred), 4),
            round(metrics.recall_score(test[target].values, pred), 4),
            round(metrics.precision_score(test[target].values, pred), 4)
            )




